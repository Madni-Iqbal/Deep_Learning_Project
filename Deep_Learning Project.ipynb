{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb56cb3",
   "metadata": {},
   "source": [
    "# Deep Learning Project - STL-10 Dataset Classification\n",
    "\n",
    "## Project Requirements:\n",
    "1. Dataset: STL-10 (Similar to CIFAR-10 but different dataset)\n",
    "2. Task: Classification\n",
    "3. EDA (Exploratory Data Analysis)\n",
    "4. Data Preprocessing\n",
    "5. Deep Learning Model (ResNet-32)\n",
    "6. Validation Loss Reduction\n",
    "7. Learning Rate Scheduling\n",
    "8. Early Stopping\n",
    "9. Data Augmentation\n",
    "10. Classification Report & Accuracy\n",
    "11. RandomSearchCV for Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow_datasets as tfds\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b72a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print versions for reference\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"Keras: {keras.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a46f5",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading - STL-10\n",
    "\n",
    "STL-10 dataset contains 13,000 color images in 10 classes (airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck). \n",
    "Images are 96x96 pixels, similar to CIFAR-10 but a different dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load STL-10 dataset - Simple and Short (No Loops!)\n",
    "print(\"Loading STL-10 dataset...\")\n",
    "\n",
    "# Load and combine all data at once\n",
    "ds_train, ds_test = tfds.load('stl10', split=['train', 'test'], as_supervised=True, batch_size=-1)\n",
    "X_all = np.concatenate([ds_train[0].numpy(), ds_test[0].numpy()], axis=0)\n",
    "y_all = np.concatenate([ds_train[1].numpy().flatten(), ds_test[1].numpy().flatten()], axis=0)\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_all, y_all, test_size=0.3, random_state=42, stratify=y_all)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Resize first\n",
    "X_train = tf.image.resize(X_train.astype('float32'), [32, 32]).numpy()\n",
    "X_val = tf.image.resize(X_val.astype('float32'), [32, 32]).numpy()\n",
    "X_test = tf.image.resize(X_test.astype('float32'), [32, 32]).numpy()\n",
    "\n",
    "# Normalize after resize\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(f\"Train: {len(X_train)} (70%) | Val: {len(X_val)} (15%) | Test: {len(X_test)} (15%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7598e",
   "metadata": {},
   "source": [
    "## 2. Task Identification: Classification\n",
    "\n",
    "This is a **Multi-class Classification** problem with 10 classes:\n",
    "- 0: airplane\n",
    "- 1: bird\n",
    "- 2: car\n",
    "- 3: cat\n",
    "- 4: deer\n",
    "- 5: dog\n",
    "- 6: horse\n",
    "- 7: monkey\n",
    "- 8: ship\n",
    "- 9: truck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbb2bd",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23174963",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Sample Images Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa024287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Display sample images from each class\n",
    "class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Sample Images from Each Class', fontsize=16)\n",
    "\n",
    "# Get first image of each class - Direct access (No loops!)\n",
    "idx0 = np.where(y_train == 0)[0][0]; idx1 = np.where(y_train == 1)[0][0]\n",
    "idx2 = np.where(y_train == 2)[0][0]; idx3 = np.where(y_train == 3)[0][0]\n",
    "idx4 = np.where(y_train == 4)[0][0]; idx5 = np.where(y_train == 5)[0][0]\n",
    "idx6 = np.where(y_train == 6)[0][0]; idx7 = np.where(y_train == 7)[0][0]\n",
    "idx8 = np.where(y_train == 8)[0][0]; idx9 = np.where(y_train == 9)[0][0]\n",
    "\n",
    "# Display images with class names - Direct (No loops!)\n",
    "axes[0, 0].imshow(X_train[idx0]); axes[0, 0].set_title(f'Class 0: {class_names[0]}'); axes[0, 0].axis('off')\n",
    "axes[0, 1].imshow(X_train[idx1]); axes[0, 1].set_title(f'Class 1: {class_names[1]}'); axes[0, 1].axis('off')\n",
    "axes[0, 2].imshow(X_train[idx2]); axes[0, 2].set_title(f'Class 2: {class_names[2]}'); axes[0, 2].axis('off')\n",
    "axes[0, 3].imshow(X_train[idx3]); axes[0, 3].set_title(f'Class 3: {class_names[3]}'); axes[0, 3].axis('off')\n",
    "axes[0, 4].imshow(X_train[idx4]); axes[0, 4].set_title(f'Class 4: {class_names[4]}'); axes[0, 4].axis('off')\n",
    "axes[1, 0].imshow(X_train[idx5]); axes[1, 0].set_title(f'Class 5: {class_names[5]}'); axes[1, 0].axis('off')\n",
    "axes[1, 1].imshow(X_train[idx6]); axes[1, 1].set_title(f'Class 6: {class_names[6]}'); axes[1, 1].axis('off')\n",
    "axes[1, 2].imshow(X_train[idx7]); axes[1, 2].set_title(f'Class 7: {class_names[7]}'); axes[1, 2].axis('off')\n",
    "axes[1, 3].imshow(X_train[idx8]); axes[1, 3].set_title(f'Class 8: {class_names[8]}'); axes[1, 3].axis('off')\n",
    "axes[1, 4].imshow(X_train[idx9]); axes[1, 4].set_title(f'Class 9: {class_names[9]}'); axes[1, 4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ea14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize class distribution for all datasets\n",
    "class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "class_counts = np.bincount(y_train)\n",
    "plt.bar(range(10), class_counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Training Set (70%)')\n",
    "plt.xticks(range(10), class_names, rotation=45, ha='right')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "class_counts_val = np.bincount(y_val)\n",
    "plt.bar(range(10), class_counts_val, color='lightgreen', edgecolor='black')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Validation Set (15%)')\n",
    "plt.xticks(range(10), class_names, rotation=45, ha='right')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "class_counts_test = np.bincount(y_test)\n",
    "plt.bar(range(10), class_counts_test, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Test Set (15%)')\n",
    "plt.xticks(range(10), class_names, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Print class distribution summary\n",
    "print(\"=== Class Distribution Summary ===\")\n",
    "print(\"Training set:\")\n",
    "print(f\"Class 0 ({class_names[0]}): {class_counts[0]} samples\")\n",
    "print(f\"Class 1 ({class_names[1]}): {class_counts[1]} samples\")\n",
    "print(f\"Class 2 ({class_names[2]}): {class_counts[2]} samples\")\n",
    "print(f\"Class 3 ({class_names[3]}): {class_counts[3]} samples\")\n",
    "print(f\"Class 4 ({class_names[4]}): {class_counts[4]} samples\")\n",
    "print(f\"Class 5 ({class_names[5]}): {class_counts[5]} samples\")\n",
    "print(f\"Class 6 ({class_names[6]}): {class_counts[6]} samples\")\n",
    "print(f\"Class 7 ({class_names[7]}): {class_counts[7]} samples\")\n",
    "print(f\"Class 8 ({class_names[8]}): {class_counts[8]} samples\")\n",
    "print(f\"Class 9 ({class_names[9]}): {class_counts[9]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06071c",
   "metadata": {},
   "source": [
    "### 3.3 Dataset Basic Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236febb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Dataset basic information\n",
    "print(\"=== Dataset Basic Information ===\")\n",
    "print(f\"Total samples: {len(X_all)}\")\n",
    "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X_all)*100:.1f}%)\")\n",
    "print(f\"Validation samples: {len(X_val)} ({len(X_val)/len(X_all)*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(X_test)} ({len(X_test)/len(X_all)*100:.1f}%)\")\n",
    "print(f\"\\nImage shape: {X_train[0].shape}\")\n",
    "print(f\"Image dimensions: {X_train.shape[1]}x{X_train.shape[2]}x{X_train.shape[3]}\")\n",
    "print(f\"Data type: {X_train.dtype}\")\n",
    "print(f\"\\nUnique classes: {len(np.unique(y_train))}\")\n",
    "print(f\"Class labels: {np.unique(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad8112",
   "metadata": {},
   "source": [
    "### 3.4 Dataset Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Dataset statistics\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Pixel value range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"Mean pixel value: {X_train.mean():.4f}\")\n",
    "print(f\"Std pixel value: {X_train.std():.4f}\")\n",
    "print(f\"Median pixel value: {np.median(X_train):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b0cb8",
   "metadata": {},
   "source": [
    "### 3.5 Missing Values Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aedc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Check for missing values\n",
    "print(\"=== Missing Values Check ===\")\n",
    "print(f\"Missing values in train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"Missing values in val: {np.isnan(X_val).sum()}\")\n",
    "print(f\"Missing values in test: {np.isnan(X_test).sum()}\")\n",
    "print(f\"\\nMissing labels in train: {np.isnan(y_train).sum()}\")\n",
    "print(f\"Missing labels in val: {np.isnan(y_val).sum()}\")\n",
    "print(f\"Missing labels in test: {np.isnan(y_test).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ab894",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing - Simple and Easy\n",
    "print(\"=== Data Preprocessing ===\")\n",
    "\n",
    "# Images: Already normalized (Min-Max: divided by 255.0) - Done in Cell 4\n",
    "# Labels: Convert to One-Hot Encoding\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_categorical = keras.utils.to_categorical(y_val, 10)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Images normalized: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"Labels one-hot encoded: Train {y_train_categorical.shape} | Val {y_val_categorical.shape} | Test {y_test_categorical.shape}\")\n",
    "print(\"=== Preprocessing Complete ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df7c6b",
   "metadata": {},
   "source": [
    "## 5. ResNet-32 Model Architecture\n",
    "\n",
    "This section includes:\n",
    "- Data Augmentation\n",
    "- ResNet-32 Model Building\n",
    "- Learning Rate Scheduling\n",
    "- Early Stopping\n",
    "- Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a0ddc",
   "metadata": {},
   "source": [
    "### 5.1 Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation - Simple\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "print(\"Data Augmentation: Random Flip, Rotation, Zoom\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e2205",
   "metadata": {},
   "source": [
    "### 5.2 ResNet-32 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d641ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-32 Model - Simple and Effective\n",
    "def residual_block(x, filters):\n",
    "    shortcut = x\n",
    "    x = layers.Conv2D(filters, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(filters, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, 1)(shortcut)\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Build ResNet-32\n",
    "inputs = layers.Input(shape=(32, 32, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Conv2D(32, 3, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "# ResNet blocks (32 layers total)\n",
    "x = residual_block(x, 32)\n",
    "x = residual_block(x, 32)\n",
    "x = residual_block(x, 32)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "x = residual_block(x, 64)\n",
    "x = residual_block(x, 64)\n",
    "x = residual_block(x, 64)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "x = residual_block(x, 128)\n",
    "x = residual_block(x, 128)\n",
    "x = residual_block(x, 128)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"ResNet-32 Model Created\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437b7a7",
   "metadata": {},
   "source": [
    "### 5.3 Callbacks (Learning Rate Scheduling + Early Stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ced4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduling + Early Stopping\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "callbacks_list = [lr_scheduler, early_stopping]\n",
    "print(\"Callbacks: Learning Rate Scheduling + Early Stopping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d9ef1",
   "metadata": {},
   "source": [
    "### 5.4 Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a75d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "print(\"Training ResNet-32...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical,\n",
    "    validation_data=(X_val, y_val_categorical),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbac60e",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a8ce9",
   "metadata": {},
   "source": [
    "### 6.1 Training History Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b56636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training History\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342f85b",
   "metadata": {},
   "source": [
    "### 6.2 Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89be012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78411bf2",
   "metadata": {},
   "source": [
    "### 6.3 Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b29ca",
   "metadata": {},
   "source": [
    "### 6.4 Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba59a3",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning (RandomSearchCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearchCV for Hyperparameter Tuning - Simple and Working\n",
    "print(\"=== RandomSearchCV Hyperparameter Tuning ===\")\n",
    "\n",
    "# Define model creation function\n",
    "def create_model_for_search(learning_rate=0.001, dropout_rate=0.5, dense_units=128):\n",
    "    def residual_block(x, filters):\n",
    "        shortcut = x\n",
    "        x = layers.Conv2D(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1)(shortcut)\n",
    "        x = layers.Add()([x, shortcut])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = layers.Conv2D(32, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = residual_block(x, 32)\n",
    "    x = residual_block(x, 32)\n",
    "    x = residual_block(x, 32)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Parameter grid for RandomSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'dense_units': [64, 128, 256]\n",
    "}\n",
    "\n",
    "# Use subset for faster search\n",
    "X_train_subset = X_train[:2000]\n",
    "y_train_subset = y_train_categorical[:2000]\n",
    "X_val_subset = X_val[:500]\n",
    "y_val_subset = y_val_categorical[:500]\n",
    "\n",
    "# Manual RandomSearchCV (Simple approach)\n",
    "print(\"Running RandomSearchCV (5 random combinations)...\")\n",
    "results = []\n",
    "\n",
    "# Randomly sample 5 combinations\n",
    "np.random.seed(42)\n",
    "for i in range(5):\n",
    "    lr = np.random.choice(param_grid['learning_rate'])\n",
    "    dr = np.random.choice(param_grid['dropout_rate'])\n",
    "    du = np.random.choice(param_grid['dense_units'])\n",
    "    \n",
    "    print(f\"\\nTrial {i+1}/5: LR={lr}, Dropout={dr}, Dense={du}\")\n",
    "    temp_model = create_model_for_search(learning_rate=lr, dropout_rate=dr, dense_units=du)\n",
    "    \n",
    "    temp_model.fit(X_train_subset, y_train_subset, \n",
    "                   validation_data=(X_val_subset, y_val_subset),\n",
    "                   epochs=10, batch_size=64, verbose=0)\n",
    "    \n",
    "    score = temp_model.evaluate(X_val_subset, y_val_subset, verbose=0)[1]\n",
    "    results.append({'learning_rate': lr, 'dropout_rate': dr, 'dense_units': du, 'score': score})\n",
    "    print(f\"  Validation Accuracy: {score:.4f}\")\n",
    "\n",
    "# Find best parameters\n",
    "best_result = max(results, key=lambda x: x['score'])\n",
    "print(\"\\n=== RandomSearchCV Results ===\")\n",
    "print(f\"Best Parameters: {best_result}\")\n",
    "print(f\"\\nAll Results:\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"  {i+1}. LR={r['learning_rate']}, Dropout={r['dropout_rate']}, Dense={r['dense_units']} - Score: {r['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de6613",
   "metadata": {},
   "source": [
    "## 8. Project Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Project Summary\n",
    "print(\"=\"*60)\n",
    "print(\"DEEP LEARNING PROJECT - STL-10 CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: STL-10\")\n",
    "print(f\"Total Samples: {len(X_all)}\")\n",
    "print(f\"Train: {len(X_train)} (70%) | Val: {len(X_val)} (15%) | Test: {len(X_test)} (15%)\")\n",
    "print(f\"\\nModel: ResNet-32\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nFeatures Used:\")\n",
    "print(\"  ✓ Data Augmentation (Flip, Rotation, Zoom)\")\n",
    "print(\"  ✓ Learning Rate Scheduling (ReduceLROnPlateau)\")\n",
    "print(\"  ✓ Early Stopping\")\n",
    "print(\"  ✓ Batch Normalization\")\n",
    "print(\"  ✓ Dropout Regularization\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fe132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d2e32",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bff45",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d82eea",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d428f",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26347c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e392b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
